{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigVGAN Vocoder Fine-tuning\n",
    "\n",
    "Fine-tune BigVGAN v2 on kick drum samples using a Colab T4 GPU.\n",
    "\n",
    "1. Upload your `data/kicks/` folder to Google Drive\n",
    "2. Run all cells\n",
    "3. Download the fine-tuned weights from Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "!pip install -q torch==2.6.0 torchaudio==2.6.0 bigvgan==2.4.1 huggingface_hub==0.36.2",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Path to your kick samples in Google Drive\n",
    "DATA_DIR = \"/content/drive/MyDrive/kicks/data/kicks\"\n",
    "# Where to save fine-tuned weights\n",
    "SAVE_DIR = \"/content/drive/MyDrive/kicks/models/vocoder\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import itertools\nimport os\n\nimport torch\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import GradScaler, autocast\n\nimport bigvgan\nfrom bigvgan.discriminators import (\n    MultiPeriodDiscriminator,\n    MultiScaleSubbandCQTDiscriminator,\n)\nfrom bigvgan.loss import (\n    generator_loss,\n    discriminator_loss,\n    feature_loss,\n    MultiScaleMelSpectrogramLoss,\n)\nfrom bigvgan.meldataset import mel_spectrogram\n\nSAMPLE_RATE = 44100\nAUDIO_LENGTH = 65536\nVOCODER_MODEL = \"nvidia/bigvgan_v2_44khz_128band_256x\"\n\n\nclass KickAudioDataset(Dataset):\n    def __init__(self, dir: str) -> None:\n        self.waveforms: list[torch.Tensor] = []\n        for file in sorted(os.listdir(dir)):\n            if not file.endswith('.wav'):\n                continue\n            audio, sr = torchaudio.load(os.path.join(dir, file))\n            if audio.shape[0] > 1:\n                audio = torch.mean(audio, dim=0, keepdim=True)\n            if sr != SAMPLE_RATE:\n                audio = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(audio)\n            if audio.shape[-1] > AUDIO_LENGTH:\n                audio = audio[:, :AUDIO_LENGTH]\n            elif audio.shape[-1] < AUDIO_LENGTH:\n                audio = torch.nn.functional.pad(audio, (0, AUDIO_LENGTH - audio.shape[-1]))\n            audio = audio / (audio.abs().max() + 1e-8)\n            self.waveforms.append(audio.squeeze(0))\n        print(f'Loaded {len(self.waveforms)} kick samples')\n\n    def __len__(self) -> int:\n        return len(self.waveforms)\n\n    def __getitem__(self, idx: int) -> torch.Tensor:\n        return self.waveforms[idx]\n\n\ndef finetune(\n    data_dir: str = DATA_DIR,\n    save_dir: str = SAVE_DIR,\n    epochs: int = 200,\n    batch_size: int = 2,\n    lr: float = 1e-4,\n    grad_accum: int = 4,\n    save_every: int = 50,\n) -> None:\n    os.makedirs(save_dir, exist_ok=True)\n    device = torch.device('cuda')\n    print(f'GPU: {torch.cuda.get_device_name()}')\n\n    # Load pretrained generator\n    generator = bigvgan.BigVGAN.from_pretrained(VOCODER_MODEL, use_cuda_kernel=False)\n    h = generator.h\n    generator = generator.train().to(device)\n\n    # Freeze early layers â€” only fine-tune last 2 upsampling stages + output\n    for param in generator.parameters():\n        param.requires_grad = False\n    num_ups = len(generator.ups)\n    for i in range(max(0, num_ups - 2), num_ups):\n        for param in generator.ups[i].parameters():\n            param.requires_grad = True\n        for j in range(generator.num_kernels):\n            for param in generator.resblocks[i * generator.num_kernels + j].parameters():\n                param.requires_grad = True\n    for param in generator.conv_post.parameters():\n        param.requires_grad = True\n    if hasattr(generator, 'activation_post'):\n        for param in generator.activation_post.parameters():\n            param.requires_grad = True\n\n    trainable_g = [p for p in generator.parameters() if p.requires_grad]\n\n    # Discriminators\n    mpd = MultiPeriodDiscriminator(h).to(device)\n    cqtd = MultiScaleSubbandCQTDiscriminator(h).to(device)\n\n    mel_loss_fn = MultiScaleMelSpectrogramLoss(sampling_rate=h.sampling_rate)\n\n    optim_g = torch.optim.AdamW(trainable_g, lr=lr, betas=(h.adam_b1, h.adam_b2))\n    optim_d = torch.optim.AdamW(\n        itertools.chain(mpd.parameters(), cqtd.parameters()),\n        lr=lr, betas=(h.adam_b1, h.adam_b2),\n    )\n    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(optim_g, gamma=h.lr_decay)\n    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(optim_d, gamma=h.lr_decay)\n    scaler = GradScaler()\n\n    dataset = KickAudioDataset(data_dir)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n\n    gen_total = sum(p.numel() for p in generator.parameters())\n    gen_train = sum(p.numel() for p in trainable_g)\n    print(f'Generator: {gen_total:,} params ({gen_train:,} trainable)')\n    print(f'MPD: {sum(p.numel() for p in mpd.parameters()):,} params')\n    print(f'CQT-D: {sum(p.numel() for p in cqtd.parameters()):,} params')\n    print(f'Training for {epochs} epochs, batch_size={batch_size}, grad_accum={grad_accum}')\n\n    best_mel_loss = float('inf')\n\n    for epoch in range(1, epochs + 1):\n        epoch_g_loss = 0.0\n        epoch_d_loss = 0.0\n        epoch_mel = 0.0\n        n_batches = 0\n\n        for step, wav in enumerate(dataloader):\n            wav = wav.to(device).unsqueeze(1)\n            mel = mel_spectrogram(\n                wav.squeeze(1), h.n_fft, h.num_mels,\n                h.sampling_rate, h.hop_size, h.win_size,\n                h.fmin, h.fmax, center=False,\n            ).to(device)\n\n            # -- Discriminator step --\n            with autocast():\n                with torch.no_grad():\n                    wav_gen_d = generator(mel)\n                y_df_hat_r, y_df_hat_g, _, _ = mpd(wav, wav_gen_d)\n                loss_disc_f, _, _ = discriminator_loss(y_df_hat_r, y_df_hat_g)\n                y_ds_hat_r, y_ds_hat_g, _, _ = cqtd(wav, wav_gen_d)\n                loss_disc_s, _, _ = discriminator_loss(y_ds_hat_r, y_ds_hat_g)\n\n            loss_d_val = loss_disc_f.item() + loss_disc_s.item()\n            scaler.scale((loss_disc_f + loss_disc_s) / grad_accum).backward()\n            del wav_gen_d, y_df_hat_r, y_df_hat_g, y_ds_hat_r, y_ds_hat_g\n            del loss_disc_f, loss_disc_s\n            torch.cuda.empty_cache()\n\n            # -- Generator step --\n            with autocast():\n                wav_gen = generator(mel)\n                loss_mel = mel_loss_fn(wav, wav_gen) * h.lambda_melloss\n                mel_val = loss_mel.item()\n                y_df_hat_r, y_df_hat_g, fmap_f_r, fmap_f_g = mpd(wav, wav_gen)\n                loss_gen_f, _ = generator_loss(y_df_hat_g)\n                loss_fm_f = feature_loss(fmap_f_r, fmap_f_g)\n                y_ds_hat_r, y_ds_hat_g, fmap_s_r, fmap_s_g = cqtd(wav, wav_gen)\n                loss_gen_s, _ = generator_loss(y_ds_hat_g)\n                loss_fm_s = feature_loss(fmap_s_r, fmap_s_g)\n\n            loss_g_val = loss_gen_f.item() + loss_gen_s.item() + loss_fm_f.item() + loss_fm_s.item() + mel_val\n            scaler.scale((loss_gen_f + loss_gen_s + loss_fm_f + loss_fm_s + loss_mel) / grad_accum).backward()\n            del wav_gen, y_df_hat_r, y_df_hat_g, y_ds_hat_r, y_ds_hat_g\n            del fmap_f_r, fmap_f_g, fmap_s_r, fmap_s_g\n            del loss_gen_f, loss_gen_s, loss_fm_f, loss_fm_s, loss_mel\n            torch.cuda.empty_cache()\n\n            if (step + 1) % grad_accum == 0 or (step + 1) == len(dataloader):\n                scaler.unscale_(optim_d)\n                torch.nn.utils.clip_grad_norm_(\n                    itertools.chain(mpd.parameters(), cqtd.parameters()), h.clip_grad_norm)\n                scaler.step(optim_d)\n                optim_d.zero_grad()\n\n                scaler.unscale_(optim_g)\n                torch.nn.utils.clip_grad_norm_(trainable_g, h.clip_grad_norm)\n                scaler.step(optim_g)\n                optim_g.zero_grad()\n\n                scaler.update()\n\n            epoch_g_loss += loss_g_val\n            epoch_d_loss += loss_d_val\n            epoch_mel += mel_val\n            n_batches += 1\n\n        scheduler_g.step()\n        scheduler_d.step()\n\n        avg_g = epoch_g_loss / max(n_batches, 1)\n        avg_d = epoch_d_loss / max(n_batches, 1)\n        avg_mel = epoch_mel / max(n_batches, 1)\n\n        if epoch % 10 == 0 or epoch == 1:\n            print(f'Epoch {epoch:>3d}/{epochs}  G={avg_g:.3f}  D={avg_d:.3f}  Mel={avg_mel:.3f}')\n\n        if avg_mel < best_mel_loss:\n            best_mel_loss = avg_mel\n            torch.save({\n                'generator': generator.state_dict(),\n                'epoch': epoch,\n                'mel_loss': avg_mel,\n            }, os.path.join(save_dir, 'best.pth'))\n\n        if epoch % save_every == 0:\n            torch.save({\n                'generator': generator.state_dict(),\n                'mpd': mpd.state_dict(),\n                'cqtd': cqtd.state_dict(),\n                'optim_g': optim_g.state_dict(),\n                'optim_d': optim_d.state_dict(),\n                'epoch': epoch,\n            }, os.path.join(save_dir, f'checkpoint_{epoch}.pth'))\n            print(f'  Saved checkpoint at epoch {epoch}')\n\n    torch.save({\n        'generator': generator.state_dict(),\n        'epoch': epochs,\n        'mel_loss': best_mel_loss,\n    }, os.path.join(save_dir, 'final.pth'))\n\n    print(f'\\nDone! Best mel loss: {best_mel_loss:.4f}')\n    print(f'Weights saved to {save_dir}/')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "finetune()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download `best.pth` from your Drive folder and place it at `models/vocoder/best.pth` in your local project."
   ]
  }
 ]
}